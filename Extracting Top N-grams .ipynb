{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text cleaning \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile(\"[/(){}\\[\\]\\|@,;!]\")\n",
    "BAD_SYMBOLS_RE = re.compile(\"[^0-9a-z #+_]\")\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "def text_preprocess(text):\n",
    "    \"\"\" Preprocess the input text and returns clean text\n",
    "    Args:\n",
    "        text (str): Input string\n",
    "    \n",
    "    returns:\n",
    "        Returns cleaned string\n",
    "        \n",
    "    \"\"\"\n",
    "    # removing digits\n",
    "    text = text.replace(\"\\d+\",\" \")\n",
    "    \n",
    "    # removing mentions and urls\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text) \n",
    "    \n",
    "    # lowercase text\n",
    "    text = text.lower() \n",
    "    \n",
    "    # removing digits\n",
    "    text =  re.sub('[0-9]+', '', text)\n",
    "    \n",
    "    # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(\" \", text) \n",
    "    \n",
    "    # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = BAD_SYMBOLS_RE.sub(\" \", text) \n",
    "    \n",
    "    # delete stopwors from text\n",
    "    text = ' '.join([word for word in text.split() if word not in stop]) \n",
    "    \n",
    "    # strip any white space characters\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_ngrams(scoring, corpus, ngram = (1,1), descending=True):\n",
    "    ''' This function returns the most  or least frequent/important ngrams from the text corpus\n",
    "    \n",
    "    Args:\n",
    "        Scoring: Type of scroing to sort top ngrams - Frequency or Importance\n",
    "        corpus (list): Preprocessed text corpus\n",
    "        ngram (tuple, optional): Type of ngram. Default is Unigram (1,1).\n",
    "        descending (Bool, Optional): Boolean value to get most or least frequent/important ngrams\n",
    "        \n",
    "    returns:\n",
    "        Returns ngrams dataframe\n",
    "    '''\n",
    "    # Checking if the user wants Frequent or important ngrams\n",
    "    if scoring == 'Frequency':\n",
    "        # Creating count vectorizer model\n",
    "        vectorizer = CountVectorizer(max_features = 1000, ngram_range= ngram )\n",
    "        vector = vectorizer.fit_transform(corpus)\n",
    "    else:\n",
    "        # Creating the TF-IDF model\n",
    "        vectorizer = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, ngram_range= ngram, max_features=1000)\n",
    "        vector = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # Extracting the top Frequent/Important ngrams from the text corpus\n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    freq_imp = vector.toarray().sum(axis=0)\n",
    "    ngrams = zip(feat_names, freq_imp)\n",
    "    \n",
    "    if descending:\n",
    "        ngrams = sorted(ngrams, key= lambda x: x[1], reverse=True)\n",
    "        return pd.DataFrame(ngrams, columns=['ngram', 'freq/imp'])\n",
    "    else:\n",
    "        ngrams = sorted(ngrams, key = lambda x: x[1], reverse=False)\n",
    "        return pd.DataFrame(ngrams, columns=['ngram', 'freq/imp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Toy corpus to test our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_corpus = pd.DataFrame([\"\"\" So, Data Science is primarily used to make decisions and predictions making use of predictive causal analytics, prescriptive analytics (predictive plus decision science) and machine learning. Predictive causal analytics – If you want a model which can predict the possibilities of a particular event in the future, you need to apply predictive causal analytics. Say, if you are providing money on credit, then the probability of customers making future credit payments on time is a matter of concern for you. Here, you can build a model which can perform predictive analytics on the payment history of the customer to predict if the future payments will be on time or not.\n",
    "Prescriptive analytics: If you want a model which has the intelligence of taking its own decisions and the ability to modify it with dynamic parameters, you certainly need prescriptive analytics for it. This relatively new field is all about providing advice. In other terms, it not only predicts but suggests a range of prescribed actions and associated outcomes.\n",
    "The best example for this is Google’s self-driving car which I had discussed earlier too. The data gathered by vehicles can be used to train self-driving cars. You can run algorithms on this data to bring intelligence to it. This will enable your car to take decisions like when to turn, which path to take, when to slow down or speed up.\n",
    "Machine learning for making predictions — If you have transactional data of a finance company and need to build a model to determine the future trend, then machine learning algorithms are the best bet. This falls under the paradigm of supervised learning. It is called supervised because you already have the data based on which you can train your machines. For example, a fraud detection model can be trained using a historical record of fraudulent purchases.\n",
    "Machine learning for pattern discovery — If you don’t have the parameters based on which you can make predictions, then you need to find out the hidden patterns within the dataset to be able to make meaningful predictions. This is nothing but the unsupervised model as you don’t have any predefined labels for grouping. The most common algorithm used for pattern discovery is Clustering.\n",
    "Let’s say you are working in a telephone company and you need to establish a network by putting towers in a region. Then, you can use the clustering technique to find those tower locations which will ensure that all the users receive optimum signal strength. \"\"\"], columns=['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>So, Data Science is primarily used to make de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0   So, Data Science is primarily used to make de..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Text Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>So, Data Science is primarily used to make de...</td>\n",
       "      <td>data science primarily used make decisions pre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0   So, Data Science is primarily used to make de...   \n",
       "\n",
       "                                        Text Cleaned  \n",
       "0  data science primarily used make decisions pre...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning the toy corpus using our custom text preprocessing function\n",
    "toy_corpus['Text Cleaned'] = toy_corpus['Text'].astype(str).apply(text_preprocess)\n",
    "toy_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>freq/imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>machine learning</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>causal analytics</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>predictive causal</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>prescriptive analytics</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>analytics want</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>users receive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>using historical</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>vehicles used</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>within dataset</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>working telephone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ngram  freq/imp\n",
       "0          machine learning         4\n",
       "1          causal analytics         3\n",
       "2         predictive causal         3\n",
       "3    prescriptive analytics         3\n",
       "4            analytics want         2\n",
       "..                      ...       ...\n",
       "190           users receive         1\n",
       "191        using historical         1\n",
       "192           vehicles used         1\n",
       "193          within dataset         1\n",
       "194       working telephone         1\n",
       "\n",
       "[195 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the custom function to get the most frequent ngrams\n",
    "top_ngrams(\"Frequency\", toy_corpus['Text Cleaned'].tolist(), descending=True, ngram=(2,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
