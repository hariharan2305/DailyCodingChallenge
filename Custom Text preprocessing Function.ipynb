{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP: Custom Text Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile(\"[/(){}\\[\\]\\|@,;!]\")\n",
    "BAD_SYMBOLS_RE = re.compile(\"[^0-9a-z #+_]\")\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "def text_preprocess(text):\n",
    "    \"\"\" Preprocess the input text and returns clean text\n",
    "    Args:\n",
    "        text (str): Input string\n",
    "    \n",
    "    returns:\n",
    "        Returns cleaned string\n",
    "        \n",
    "    \"\"\"\n",
    "    # removing digits\n",
    "    text = text.replace(\"\\d+\",\" \")\n",
    "    \n",
    "    # removing mentions and urls\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text) \n",
    "    \n",
    "    # lowercase text\n",
    "    text = text.lower() \n",
    "    \n",
    "    # removing digits\n",
    "    text =  re.sub('[0-9]+', '', text)\n",
    "    \n",
    "    # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(\" \", text) \n",
    "    \n",
    "    # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = BAD_SYMBOLS_RE.sub(\" \", text) \n",
    "    \n",
    "    # delete stopwors from text\n",
    "    text = ' '.join([word for word in text.split() if word not in stop]) \n",
    "    \n",
    "    # strip any white space characters\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working of the function using simple toy corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_corpus = \"\"\"The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s when the first statistical machine translation systems were developed. refer the link https://pypi.org/project/nlppreprocess/#history\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the number of words before and after the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy Corpus Raw: 654 words \n",
      "\n",
      " The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s when the first statistical machine translation systems were developed. refer the link https://pypi.org/project/nlppreprocess/#history\n",
      "\n",
      "\n",
      "Toy Corpus Cleaned (Custom regex function): 442 words \n",
      "\n",
      " georgetown experiment involved fully automatic translation sixty russian sentences english authors claimed within three five years machine translation would solved problem however real progress much slower alpac report found ten year long research failed fulfill expectations funding machine translation dramatically reduced little research machine translation conducted late first statistical machine translation systems developed refer link\n"
     ]
    }
   ],
   "source": [
    "print(\"Toy Corpus Raw: \" + str(len(toy_corpus))+ \" words\",\"\\n\\n\", toy_corpus)\n",
    "\n",
    "# using our function for preproceessing\n",
    "\n",
    "cleaned2 = text_preprocess(toy_corpus)\n",
    "print(\"\\n\\nToy Corpus Cleaned (Custom regex function): \" + str(len(cleaned2))+ \" words\",\"\\n\\n\", cleaned2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
